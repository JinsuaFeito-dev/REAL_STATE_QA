from llama_cpp import Llama
from pathlib import Path
from pydantic import BaseModel, ConfigDict
from yaml import safe_load
from loguru import logger
import json


class llm_model(BaseModel):
    """
    A model class for handling interactions with the Llama language model and
    formatting responses based on a template.

    Attributes
    ----------
    llama_config : dict
        Configuration for the Llama model.
    template_path : Path
        Path to the template YAML file.
    database_context : str, optional
        Context from the database to be used in the template, by default "".
    template : dict, optional
        Loaded and filled template, by default None.
    llm : Llama, optional
        Instance of the Llama model, by default None.

    Methods
    -------
    __init__(**data)
        Initializes the model with the provided configuration and checks for the existence
        of the template and model paths.
    _initialize_model()
        Initializes the Llama model using the provided configuration.
    infer(natural_language_query)
        Generates a response based on the natural language query using the Llama model.
    fill_template(database_ctx, natural_language_query)
        Loads the template from the YAML file and fills it with the provided context and query.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    llama_config: dict
    template_path: Path
    _database_context: str = ""
    template: dict = None
    llm: Llama = None

    def __init__(self, **data):
        """
        Initializes the model with the provided configuration.

        Parameters
        ----------
        **data : dict
            Configuration data for the model.

        Raises
        ------
        FileNotFoundError
            If the template path or model path does not exist.
        """
        super().__init__(**data)
        if not self.template_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.template_path}")

        self.llama_config["model_path"] = Path(
            self.llama_config["model_path"]
        ).absolute()
        if not self.llama_config["model_path"].exists():
            path = self.llama_config["model_path"]
            raise FileNotFoundError(f"Config file not found: {path}")

        self.llama_config["model_path"] = str(self.llama_config["model_path"])

    @property
    def database_context(self) -> str:
        return self._database_context

    @database_context.setter
    def database_context(self, value: str):
        self._database_context = value
        logger.info(f"Database context set: {value}")

    def _initialize_model(self):
        """
        Initializes the Llama model using the provided configuration.
        """
        try:
            self.llm = Llama(**self.llama_config)
            logger.info(f"Llama model initialized correctly")
        except Exception as e:
            logger.error(f"Error initializing Llama model: {e}")
            raise

    def infer(self, natural_language_query):
        """
        Generates a response based on the natural language query using the Llama model.

        Parameters
        ----------
        natural_language_query : str
            The user's query in natural language.

        Returns
        -------
        str
            The response generated by the Llama model.
        """
        try:
            self.fill_template(self.database_context, natural_language_query)
            answer = self.llm.create_chat_completion(
                messages=self.template["messages"],
                response_format=self.template["response_format"],
                temperature=self.template["temperature"],
            )
            try:
                answer_json = json.loads(
                    answer["choices"][0]["message"]["content"].replace("\n", " ")
                )
            except Exception as e:
                logger.error(f"Error parsing Llama response: {e}")
                answer_json = ({"sql_query": "Error"},)
            return answer_json["sql_query"]
        except Exception as e:
            logger.error(f"Error inferring response: {e}")
            raise

    def fill_template(self, database_ctx, natural_language_query):
        """
        Loads the template from the YAML file and fills it with the provided context and query.

        Parameters
        ----------
        database_ctx : str
            The context from the database.
        natural_language_query : str
            The user's query in natural language.
        """
        try:
            # Load the template from the YAML file
            with open(self.template_path, "r") as f:
                template = safe_load(f)

            # Fill the template with the variables
            for message in template["messages"]:
                if message["role"] == "system":
                    context = database_ctx["system"] if "system" in database_ctx else ""
                elif message["role"] == "user":
                    context = database_ctx["user"] if "user" in database_ctx else ""
                else:
                    context = ""

                if "{database_ctx}" in message["content"]:
                    message["content"] = message["content"].replace(
                        "{database_ctx}", context
                    )
                if "{natural_language_query}" in message["content"]:
                    message["content"] = message["content"].replace(
                        "{natural_language_query}", natural_language_query
                    )

            self.template = template
            logger.info("Template filled successfully")
        except Exception as e:
            logger.error(f"Error filling template: {e}")
            raise
